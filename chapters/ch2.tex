\documentclass[../main.tex]{subfiles} \label{chapter_soa}
\begin{document}

In this chapter, we aim to give a literature review on how to build relational structures. Our main goal is to outline a general pipeline that allows us to execute this task. In order to do so, we are also concerned by giving a broad catalogue of techniques by means of which our pipeline can be implemented.
\par Different relational structures serve to different purposes. We are interested in building one that models the dynamic nature of bird species relations, which are affected not only by evolution and diversification, but also by behavioural phenomena such as flock migration.
\par However, limiting ourselves to works focusing exclusively on building structures might be too restrictive: results from bird classification, clustering and detection could also provide insight to our project. For example, feature extraction is common to all tasks, and state-of-the-art feature extraction is mandatory in order to achieve better results. Therefore, whenever relevant, works related to these tasks will also be cited in this review.
\par This chapter is presented as follows: firstly, a general overview of pipelines for building relational structures is given; afterwards, we present a review of feature extraction methods used in acoustic signals; algorithms for building relational structures are reviewed next; finally, a discussion on how the referenced methods are relevant a working pipeline for our scenario is presented.
\par The final products of this chapter do not only comprise a pipeline to build relational structures, but also informed choices on how to perform each of its stages. We explore several methods for each stage and aim to present the advantages and disadvantages of each of them. This enables us to choose a method for each stage of the pipeline, each of which will be further explored in the next chapters \ref{chapter_formants} and \ref{chapter_hmms}. 

\section{Relational structures and pipelines}\label{general_pipeline}
A relational structure is one that shows links between objects. In this work, we will assume these links to be binary relations $R$ over a single set $X$, i.e. $R \subseteq X \times X$. One way of representing such relations is a square matrix $A$, called adjacency matrix, such that $A \in \mathbb{R}^{n \times n}$, where $n =\left\vert{X}\right\vert$.
\par In real life, information about objects can be represented as a relational structure. As the number of objects grows large, specific subsets of $R$ may present an increasingly complex topology. Complex graphs (as they are commonly called in the literature) and how to identify them have been extensively discussed in the field. In \cite{Kim2008}, the authors give a narrow definition for complex graphs, which we adapt as follows:
\theoremstyle{definition}
\begin{definition}{Complex graph}. A graph whose topological features deviate from random ones. In other words, a graph that contains many different subgraphs.
\end{definition}
\par A problem that has attracted attention in recent years is community detection. Given a graph $G = (X, E)$ depicting a relation between pairs of objects in $X$, community detection can be seen as a problem of clustering the elements in $X$, i.e. partitioning the vertices $X$ in such a way that the edges between those in different clusters are comparatively fewer than those between those belonging to the same cluster \cite{Fortunato2010}. 
\par Clustering is the task of ``grouping or segmenting a collection of objects into subsets or clusters, such that those within each cluster are more closely related to one another than objects assigned to different clusters" \cite{hastie2008}. An example of clustering for the 2-dimensional case can be seen in \ref{clustering}.
\begin{figure}[ht]
\centering
\includegraphics{clustering}
\caption{Simulated data in the plane, clustered into three classes. Image taken from \cite{hastie2008}.}
\label{clustering}
\end{figure}
\par Therefore, performing clustering requires computing the \emph{closeness} of elements in the set of objects $X$. Proximity, closeness, similarity or dissimilarity, all refer to \emph{distance functions} or \emph{metrics}. 
\theoremstyle{definition}
\begin{definition}{Metric}.
\label{def_metric}
A non-negative function $d$ satisfying:
\begin{enumerate}
\item Triangle inequality. $d(x, z) \leq d(x, y) + d(y, z) $
\item Symmetry. $d(x, y) = d(y, x)$
\item $d(x, x) = 0$
\item $d(x, y) = 0 \iff x = y$
\end{enumerate}
\end{definition}
\par However, some authors relax the last condition when defining a similarity metric \cite{Goshtasby2012}. Two non-equal objects might still be very similar under certain conditions, so the condition $d(x, y) = 0 \implies x = y$ is dropped.
\par Thus, defining a metric requires knowledge about what kind of objects we are grouping. In Machine Learning in particular, we will also be concerned by how we transform data into objects for which we can define a metric. This process, called \emph{feature extraction}, consists in ``deriving features from raw data that can be used as input for a learning procedure" \cite{hastie2008}. The goal is to use domain knowledge to extract features that will reduce redundancy in raw data and highlight distinguishing characteristics, thus optimising the learning procedure in both, correctness and performance \cite{Bishop2006}. 
\par By working our way backwards, we have implicitly defined a general pipeline to build relational structures: algorithms that do so require the definition of a metric between objects. These objects will be the vectors or features extracted from each object of interest. A diagram depicting this procedure is shown in figure \ref{pipeline}.
\begin{figure}
\centering
\begin{tikzpicture}[node distance=2cm]
\node (start) [startstop] {Start};
\node (in1) [io, below of=start] {Raw data};
\node (pro1) [process, below of=in1] {Features and characterisation};
\node (pro2) [process, below of=pro1] {Compute similarity};
\node (in3) [io, below of=pro2] {Pairwise proximity};
\node (pro3) [process, below of=in3] {Build relational structure};
\node (out1) [io, below of=pro3] {Relational structure};
\node (stop) [startstop, below of=out1] {Stop};
\draw [arrow] (start) -- (in1);
\draw [arrow] (in1) -- (pro1);
\draw [arrow] (pro1) -- (pro2);
\draw [arrow] (pro2) -- (in3);
\draw [arrow] (in3) -- (pro3);
\draw [arrow] (pro3) -- (out1);
\draw [arrow] (out1) -- (stop);
\end{tikzpicture}
\caption{A general pipeline to build relational structures.}
\label{pipeline}
\end{figure}
\par We now tackle the rest of this literature review in stages that we can put together in order to build relational structures. We will first give an overview of the general aspects of birdsong in section \ref{birdsong_review}, aiming to present in a structured manner the data analysed in this work. After, we dedicate a section for each of the processes in the general pipeline. Section \ref{features_review} presents different procedures to extract features from birdsong automatically. Finally, section \ref{algorithms_review} gives an overview of algorithms used to generate relational structures from input vectors and metrics.

\section{General aspects of birdsong} \label{birdsong_review}
In this section, we present the concept of birdsong from a biological perspective. We will address this by giving relevant information on the acoustic structure of birdsong, and how it differs from other bird vocalisations. Moreover, we also touch on the social and behavioural functions of birdsong and their relation to human speech. Finally, we discuss some arguments regarding the evolution of birdsong through time.
\par There are two ways of acoustic communication among birds: songs and calls. The former ``resemble speech in that they are acoustically complex sequences of stereotyped vocal gestures, lasting from seconds to minutes" \cite{Snowdon2013}. On the other hand, bird calls are shorter, consonant-like sounds whose ``contribution to nature's music is minimal" \cite{Marler2004}. 
\par From a functional perspective, birdsong is used to defend territories and for mate attraction or competition \cite{Berwick2013} \cite{Naguib2014}, whereas calls are more intimately related to ``issues of life and death, such as: predator alarm, the announcement and exchange of food, and the maintenance of social proximity and group composition and integration" \cite{Marler2004}.
\par Birdsongs and bird calls are also different structurally. Biologists have proposed a hierarchical structure of birdsong: a single song is composed of multiple motifs or patterns, each of which is composed of syllables \cite{Snowdon2013}. This structure can be observed in a spectrogram\footnote{Spectrograms allow us to observe how the distribution energy-frequency changes over time for a signal. A broader discussion is offered in subsection \ref{subsection_lti}} representation in figure \ref{fig_birdsong_structure}. Bird calls, on the other hand, ``are often short, monosyllabic, with simple frequency patterning, often delivered in what often appears to be a disorderly fashion" \cite{Marler2004}. Thus, bird calls often lack the hierarchical structure and repetitive patterns that are so distinctive of birdsong. An example of this can be seen in figure \ref{fig_birdcall}.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{birdsong_structure}
\caption{Spectrogram from a birdsong recording of the species \emph{Periparus ater}. The thick blue and green bars represent syllables, and the black boxes represent a motif.}
\label{fig_birdsong_structure}
\end{figure}



\par In this work, we will be concerned exclusively with birdsong examples. Now that we have compared both types of bird communication, we will focus only on birdsong. The remaining of this section presents more relevant features of birdsong and its association with human speech and evolution.
\begin{figure}[t]
\centering
\includegraphics[width=80mm]{birdcall}
\caption{Spectrogram from a bird call recording of the species \emph{Vanillus vanillus}. Contrary to birdsong, bird call does not have a clear syllabic structure, and portions of it often contain energy quasi-uniformly spread across the frequency axis.}
\label{fig_birdcall}
\end{figure}

\par Biologists support the idea that birdsong is learnt by imitation of parent birds \cite{Berwick2013}, i.e. young birds copy many elements of elder birds to produce birdsong. Moreover, the patterns that can be produced by birds can take a long time to master, since birdsong depends on many factors, for example \cite{Naguib2014}: 
\begin{itemize}
\item Some motifs are longer than others, thus mastering them takes longer.
\item Some songs are only produced at a particular time of the day or season of the year.
\item Since one function of birdsong is mating, some songs may only be produced during the birds' fertility period.
\item The energy requirement varies among different songs.
\end{itemize}
\par The fact that birds learn to sing by imitating elder birds is a trait that has caught attention among the field scholars, due to its close relation to how humans learn to speak (by imitation as well). This is not the only similarity between both scenarios: both, birdsong and human language ``involve complex, patterned vocalisations" \cite{Berwick2013,Naguib2014}. The similarity between birdsong and human speech is also related to the auditory system of birds and humans: both hear best between 2 and 5 kHz, unlike other models of animal auditory processing \cite{Snowdon2013}. Furthermore, the bird auditory system can hear frequencies as high as 10kHz, and its human counterpart can do so up to 20kHz \cite{Snowdon2013}.
\par However, biologists consider human speech to be much more complex lexically and semantically \cite{Berwick2013}, i.e. whereas birdsong has two very clear purposes, human speech serves to many more. These functions are closely tied to the highly structured syntactic structures of natural language. Hence the phonetic complexity of both means of communication is not what makes them most distinguishable (both have syllables that make up words), but the idea that human speech adds further levels of abstraction by proceeding to assemble words into sentences, and sentences into full discourses. The morphological, syntactic and semantic complexities of human speech are virtually absent from birdsong. Nevertheless, a strong link still exists in the phonetic abstraction of language.
\par We may make yet another analogy between natural language and birdsong: human languages have evolved and diversified over time due to sociological and environmental phenomena, which researchers have tried to apply to birdsong. However, \cite{Cate2004} gives an account of this work indicating that researchers were sceptical about this theory for two reasons: on one hand, genetically related species can diverge strongly in vocal features due to flock migration - birds have to adapt their song in order to remain safe from predators and in order to be heard if their new environment is noisier. On the other hand, birds that are not genetically related might converge in their vocal features when experiencing similar environmental pressures.
\par The former is more common than the latter, and it occurs often due to acoustic changes occurring in a particular generation, and carried over to the future. Additionally, birds that migrate might be pushed to change the way they sing (or call) due to the presence of new threats. In fact, \cite{Marler2004} states that the loss of natural habitats may actually leave us forever with unanswered questions, precisely due to the missing information that would otherwise allow us to trace back the evolution of birdsong.
\par To summarise, in this section we presented general aspects of birdsong. This research is relevant for this thesis in order to establish a biological justification for the mathematical approaches described onwards. Not only does this analysis give a clearer idea of this thesis' object of study, but it also justifies the usage of signal processing techniques that have been used elsewhere. For example, giving a scientific basis that relates human speech and birdsong enables us to use some Automatic Speech Recognition techniques.
\par Although recent research has supported the idea that there is still sufficient information to perform bird species classification \cite{Naguib2014}, this is certainly not the same as building a relational structure of bird species, and the arguments above could imply that there is no theoretical guarantee (at least from the biological perspective) that such a relation could exist. This thesis aims to apply mathematical approaches to directly address this question.

\section{Feature extraction procedures for birdsong} \label{features_review}
In this section, we give an account of different birdsong feature extraction techniques used in the literature. Most of these techniques rely on extracting information from the signal in the frequency domain. As a result, we first introduce the Fourier Transform.

\subsection{The Fourier Transform and the FFT algorithm} \label{subsection_fft}
\par One widespread way of performing this domain shift is given by the Fourier transform \cite{Weisstein2015}, defined as:
\theoremstyle{definition}
\begin{definition}{Fourier Transform}.
Given a function $x(t)$, its Fourier transform $X(s)$ is defined as:
\begin{align*}
X(s) = \int_{-\infty}^{\infty}x(t)\mathrm{e}^{-2\pi ist}dx
\end{align*}
\par Where the independent variable $t$ represents time measured in seconds and $s$ represents frequency measured in Hertz.
\end{definition}
\par We can also define a discrete version of this operation:
\begin{definition}{Discrete Fourier Transform}.
Given a discrete function $\V{x} = (x_0,x_1,...,x_{N-1})$, its Fourier transform $\V{X}$ is a vector of frequency bins $X_k$ such that $\V{X} = (X_0, X_1,...,X_{N-1})$, where the $k$-th bin $X_k$ is given by:
\begin{align*}
X_k = \sum_{n=0}^{N-1}x_{n}\exp{\left(-2\pi i\frac{kn}{N}\right)}
\end{align*}
\end{definition}
\par By far the most efficient way to perform the latter is the Fast Fourier Transform (FFT), which runs in $\mathcal{O}(N\log{N})$ \cite{Smith2011}. There are several versions of this algorithm, but one of the most well-known in the literature is the Cooley-Tukey algorithm, described by the eponymous authors \cite{Cooley1965,Weisstein2015}. Its general strategy is to use intermediate results in a divide-and-conquer fashion in order to decrease the total number of operations \cite{Weisstein2015}, and it achieves so thanks to the Danielson-Lanczos Lemma:
\begin{lemma}
\emph{Danielson-Lanczos Lemma.} The DFT expansion $\V{X} = (X_1, X_2,...,X_k)$ of a discrete function $\V{x} = (x_1,x_2,...,x_n)$ can also be expressed as:\\
\begin{align*}
X_k &= \sum_{j=0}^{N/2-1}\exp{\left(-2\pi i\frac{kj}{N/2}\right)}x_{2j} + W^n\sum_{j=0}^{N/2-1}\exp{\left(-2\pi i\frac{kj}{N/2}\right)}x_{2j+1}\\
&=F^e_k + W^kF^o_k \\
\end{align*}
with
\begin{align*}
W &= \exp{\left(\frac{-2\pi i}{N}\right)}
\end{align*}
\end{lemma}
\par This is merely regrouping the addition terms as two subadditions, one along the odd-numbered indices, and one along the even-numbered indices. Moreover, by periodicity of the DFT:
\begin{align*}
F^e_k &= F^e_{k+N/2}\\
F^o_k &= F^o_{k+N/2}\\
\exp{\left(\frac{-2\pi ik}{N}\right)} &= -\exp {\left(\frac{-2\pi i(k+N/2)}{N} \right)}
\end{align*}
\par This enables us to reformulate the calculation of $X_k$ as:
\begin{align*}
X_k = F^e_k + W^kF^o_k\\
X_{k+N/2} = F^e_k - W^kF^o_k
\end{align*}
\par These reformulations reduce the calculations per recursion to nearly half of the original number, thus resulting in a much better performing procedure. More specific details of this algorithm are beyond the scope of this work, but can be consulted in \cite{Smith2011} and \cite{Cooley1965}.
\par Having discussed the transformation of signals in the time domain into signals in the frequency domain, we are now prepared to set forth different methods of feature extraction. In particular, we are interested to compute \emph{descriptive} features, i.e. to reduce the dimensionality of the original data as much as possible while minimising the amount of information lost in the process \cite{hastie2008}. Additionally, we aim to find features that will enable us to accelerate training procedures (by reducing the amount of data to process) and to encourage generalisation (by ``making it easier" to see patterns in data).

\subsection{Mel Frequency Cepstral Coefficients (MFCC)} \label{subsection_mfcc}
\par This technique is one of the most widely used in human speech recognition \cite{Jurafsky2009}, \cite{Chou2008a}, \cite{Stowell2014}. It consists in representing the spectral envelope of a frame (a portion of a signal) by analysing its Mel-frequency \emph{cepstrum}. This is done for as many overlapping frames of equal length (around 20-50 milliseconds) that can be extracted from the original signal. The Mel scale is a frequency scale created to simulate how the human ear perceives sounds \cite{Sludge2000}. Its main premise is that human ear perception of pitch is linear below 1 kHz and logarithmic above. In other words, from 1 kHz, the same amount of energy is distributed along longer ranges of frequencies. This results in the human ear having a poorer performance distinguishing very high frequencies. This pitch-invariance property has positioned them as one of the preferred audio feature extraction techniques in the research community.
\par The transformation of a variable $F$ in Hertz into Mels is given by the following transformation \cite{Lyons2014}:
\begin{align*}
F_{\text{mel}} = 1125 \log{\left(1 + \frac{F_{Hz}}{700}\right)}
\end{align*}
\par We now present the general (scale-independent) definition of the cepstrum, adapted from \cite{Gutierrez-Osuna2009}:
\begin{definition}{Cepstrum} \label{def_cepstrum}
The cepstrum $\V{c}$ of a discrete time signal $\V{x}$ is defined as the Inverse Discrete Fourier Transform (IDFT) of the log-magnitude of the frequency spectrum of $\V{x}$. In other words:
\begin{align*}
\V{c} = \mathcal{F}^{-1}\{\log{(\abs{\mathcal{F}\{\V{x}\}})}\}
\end{align*}
\end{definition}
\par Note that the absolute value $\abs{\mathcal{F}\{\V{x}\}}$ is the square root of the spectral energy of $\V{x}$, which is given by $E(\V{x}) = \abs{\mathcal{F}\{\V{x}\}}^2$. By using the knowledge from the Mel-scale, we can introduce the Mel-frequency cepstrum. 
\par The Mel-frequency cepstrum uses a frequency-domain representation in Mel-scale rather than Hertz, i.e. if $\mathcal{F_{\text{Mel}}\{\V{x}\}}$ is the spectrum of $\V{x}$ in Mel-scale, then the the Mel-frequency cepstrum $\V{c}_{\text{Mel}}$ is given by:
\begin{align*}
\V{c}_{\text{Mel}} = \mathcal{F}^{-1}\{\log{(\abs{\mathcal{F}_{\text{Mel}}\{\V{x}\}})}\}
\end{align*}
\par Given that the Mel-scale was introduced to model how the human ear detects pitch, this new estimate would show how energy in different \emph{critical bands} of frequencies is being detected by humans. A critical band is a range of frequencies centered around \emph{critical frequencies}, which are distributed according to the Mel-scale, i.e. uniformly before 1 kHz, and exponentially onwards \cite{Gutierrez-Osuna2009}. The length of this range is called the \emph{bandwidth} of the band.
\par In order to estimate the magnitude of the spectrum in Mel-scale, we compute the magnitude of the original frequency spectrum in Hertz and weight it by means of triangular filters along the frequency axis. Each filter corresponds to one critical band, is centred in a critical frequency and the length of its base corresponds to the bandwidth of the critical band, as pictured in figure \ref{mel_energy}.


\begin{figure}[t]
\includegraphics[width=\textwidth]{mel_energy}
\caption{Triangular filters to compute the spectral energy in the Mel scale. Picture taken from \cite{Sludge2000}.}
\label{mel_energy}
\end{figure}

\par Then, we continue with the steps described in definition \ref{def_cepstrum}. The result is called Mel-cepstrum and the MFCC are the first $k$ elements of the result. In practice, normally 13 coefficients are taken after using between 20 and 40 triangular filters \cite{Gutierrez-Osuna2009}. However, more recent research has shown that computers now have enough resources to use the full Mel spectrum, rather than just taking the first coefficients \cite{Stowell2014}. Furthermore, MFCC tend to be enhanced with the so-called \emph{delta features}, which represent the rate of change of the MFCC sequence over time \cite{Muda2010} \cite{Lyons2014}. Similarly, \emph{double delta features} (or acceleration) can be computed by taking the delta features of the delta sequence. 
\par In \cite{Stowell2014}, an analysis of the performance of both, raw Mel spectra and MFCC is presented and compared against other feature extraction procedures when used to characterise birdsong. The conclusion is that, although standard for human speech, there is no proof that this should be the standardised feature extractor for birdsong. The authors present unsupervised feature learning as an alternative that could be more accurate, but call for further benchmarking to be done before coming to an absolute conclusion.

\subsection{Spherical k-means} \label{subsection_spherical}
This is an unsupervised approach to feature extraction that consists in finding a normalised spanning set of $K$ vectors for the training dataset. The algorithm was first proposed by \cite{Coates2012}, first used for audio in \cite{Dieleman2013} and in particular for birdsong in \cite{Stowell2014}. It is a modification of the well-known K-means algorithm, where the main difference consists in assigning each datum to the cluster whose centroid produces the minimum cosine distance. An analogy to explain this method is that we are 'looking at the sky and clustering stars into constellations', minimising the effect of the distances between them.
\begin{definition}{Cosine distance} \label{def_cosine_distance}
The cosine distance between two vectors $\V{x}$ and $\V{y}$ separated by an angle $\theta_{\V{x}, \V{y}}$ is given by:
\begin{align*}
d(\V{x}, \V{y}) &= 1 - \cos{(\theta_{\V{x}, \V{y}})}\\
&= 1 - \frac{\V{x} \cdot \V{y}}{\norm{\V{x}}\norm{\V{y}}}
\end{align*}
\end{definition}
\par Moreover, each centroid is normalised at every iteration of the algorithm. The result is a normalised spanning set that displays the directions of greater concentration of data, as depicted in figure \ref{fig_spherical}. Normalising this spanning set is important in order to guarantee amplitude-invariance of the features. The intuition behind spherical k-means is that we project the feature vectors onto the unit sphere, and then calculate the Euclidean dissimilarity between them, hence diminishing the over-representation of high-dimensional data when calculating the Euclidean distance \cite{Hornik2012}. This projection is equivalent to finding the cosine distance between vectors. 

\begin{figure}[t]
\centering
\includegraphics[width=80mm]{spherical}
\caption{Spherical k-means applied to artificial data generated from 3 different 2D-Gaussians. The marks represent each of the normalised centroids. Image taken from \cite{Stowell2014}.}
\label{fig_spherical}
\end{figure}
\par The dot product of each datum $\V{x}_j$ with each of the vectors $\V{b}_i$ in the spanning set can be used as features $\V{x}^*_{i,j}$ for a learning procedure, with dimensionality equal to $K$. That is, let $\V{B}$ be the matrix whose rows are the vectors in the spanning set found by spherical k-means, then the new features are given by $\V{x}^*_j = \V{Bx}_j$. In \cite{Stowell2014}, spherical k-means is used as an unsupervised feature extraction procedure for birdsong recordings: the authors used Mel spectral frames (with 40 entries, as described in \ref{subsection_mfcc}) as input to the algorithm. As proposed in \cite{Dieleman2013}, all frames were normalised in length and preprocessed using PCA-whitening. More details of this procedure can be found in \cite{Stowell2014} and \cite{Dieleman2013}.

\subsection{Wavelets} \label{subsection_wavelets}
In this subsection, we give a brief overview of wavelets. This signal analysis technique has been used to analyse birdsong, showing promising results \cite{Chou2009} but at the expense of a hard-to-parametrise implementation. Wavelets are an alternative to the Short-Time Fourier Transform (STFT), discussed in subsection \ref{subsection_mfcc}, under the argument that wavelets give better quality results because ``they are localised in time and space". This last phrase means that, whereas a small change in the Fourier Transform of a function will produce changes everywhere in its time domain representation (i.e. the FT is only localised in frequency), this is not true for wavelets, since they are localised in both domains \cite{Vidakovic1991}.
\par To expand on this concept, we first introduce orthogonal wavelets.
\begin{definition}{Orthogonal wavelets.} \label{def_onwavelets}
A function $\psi\in L^2(\mathbb{R})$, the space of square integrable functions, is called an orthogonal wavelet if the family $\{\psi_{j,k}\}$ is an orthonormal basis for $L^2(\mathbb{R})$, where each $\psi_{j,k}(t) = 2^{j/2}\psi(2^jt-k)$ and $j, k \in \mathbb{Z}$.
\end{definition}
\par The definition for each $\psi_{j,k}(t)$ arises from the ideas of \emph{binary dilation} and \emph{dyadic translation}: each function has the same shape as the original $\psi(t)$, but has been scaled and translated so as to cover as much as possible from the real line \cite{Chui1992}. 
\par The simplest example of an orthogonal wavelet is given by the Haar function, $\phi_H$ \cite{Chui1992}:
\begin{displaymath}
   \psi_H(t) = \left\{
     \begin{array}{lr}
      1 & 0 \leq t < 1/2 \\
      -1 & 1/2 \leq t < 1 \\
      0 & \text{otherwise} 
     \end{array}
   \right.
\end{displaymath}
\par Consider a signal $x(t) \in L^2(\mathbb{R})$ with $\{\psi_{j,k}(t)\}$ as one of its bases. Then $x(t)$ can be expanded as an infinite linear combination:
\begin{align*}
x(t) = \sum_{j,k=-\infty}^{\infty}{c_{j,k}\psi_{j,k}(t)}, \forall j,k \in \mathbb{Z}
\end{align*}

\par Now, the signal can be expanded using a vector of coefficients $\{c_{j,k}\}$. Analogous to the Fourier Transform, we can define a wavelet transform \cite{Weisstein2015a}.
\begin{definition}{Wavelet transform.} \label{def_wtransform} The wavelet transform of a function $x(t)$ with respect to an orthogonal wavelet $\psi$ is given by:
\begin{align*}
(W_\psi x)(b, a) = \abs{a}^{-\frac{1}{2}}\int_{-\infty}^\infty x(t)\overline{\psi\left(\frac{t-b}{a}\right)}dt
\end{align*}
\end{definition}
\par And the coefficients are given by:
\begin{align*}
c_{j,k} = (W_\psi f)(\frac{k}{2^j}, \frac{1}{2^j})
\end{align*}
\par If we instead approximate the signal $x(t)$ by a finite expansion, then the finite vector of coefficients characterises the signal up to a certain accuracy, and thus can be used as features for a learning algorithm. Finding these coefficients can be solved efficiently using Mallat's Multiresolution Analysis framework. Source \cite{Vidakovic1991} can be consulted for more information on wavelet theory and algorithms.
\par Wavelets have been used successfully in acoustic signal analysis \cite{Gamulkiewicz2003}, and birdsong specifically \cite{Chou2009}. In the former, wavelet transformations are used for speech recognition: each phoneme is used as an input signal, and wavelet coefficients are extracted as features. Phoneme classification is then performed using the Dynamic Time Warping algorithm. 
\par In the latter, wavelet coefficients are used as features to characterise birdsong. However, the wavelet coefficients are not calculated directly: instead, their routine segments the original song into syllables, and then MFCC are calculated for each. The output vectors are treated as input to a wavelet transformation procedure. The result is called WMFCC (Wavelet MFCC). 
\par To conclude this subsection, it must be said that many sources have praised wavelets due to their relative strength over Fourier analysis \cite{Gamulkiewicz2003,Weisstein2015a,Chui1992,Vidakovic1991}; however, implementation issues have also been raised regarding its relative difficult of parametrisation and lack of ability to encode energy effectively \cite{Garrett-Glaser2010}.

\subsection{MUSIC algorithm} \label{subsection_music}
The MUltiple SIgnal Classification algorithm estimates a pseudospectrum of frequencies of a signal $\V{x}$ (of length $N$), so called because it depicts the peaks of the original spectrum, without giving much detail to the rest of the frequency domain. We assume that the signal can be decomposed as a linear combination of $p$ tones or signal components in the presence of Gaussian noise, i.e. $\V{x} = \V{As} + \V{v}$, where $\V{A}$ is a sample of the signal's frequencies, $\V{s}$ is a vector amplitudes, and $\V{v}$ is the vector of Gaussian noise in each component \cite{Evans}.
\par Let $\V{R_x}$ be the covariance matrix of $\V{x}$, given by $\V{R_x} = \mathbb{E}\{\V{xx}^T\}$ \cite{Evans}. Since $\V{R_x}$ is a Hermitian matrix, its eigenvectors form an orthogonal set. Moreover, when sorted by decreasing magnitude, they also represent the directions of greatest variance in the data. If we assume that the first $p$ span a \emph{signal} subspace, and that the remaining $N-p$ span a \emph{noise} subspace, then both subspaces are orthogonal \cite{Systems1989}. 
\par Now let the first $p$ eigenvectors of $\V{R}_x$ depend on $\omega$, namely
\begin{align*}
\V{e}_{\omega} = \begin{pmatrix}1&e^{i\omega}&e^{i2\omega}&...&e^{i(N-1)\omega}\end{pmatrix}, j \leq p
\end{align*}
and let the remaining $N-p$ be $\V{v}_j, p < j$. Then, there exists a value $\omega_{k}$ such that $\V{e}_{\omega_k}\bot\V{v}_j$. 
\par Furthermore, let the spectral estimate \cite{Kootsookos1999, Mathworks2015} be given by:
\begin{align*}
P_\text{MUSIC} = \frac{1}{\sum_{k=p+1}^{N}\abs{\V{v}_k^H\V{e}_{\omega}}^2}
\end{align*}
\par The peaks of this spectral estimate are given precisely when $\V{v}_k^H\V{e}_{\omega} = 0$, i.e. when $\omega = \omega_k$. By inspecting different values of $\omega$, it is possible to find such vectors and therefore store $\omega$ as the peaks of the pseudospectrum \cite{Delft2011}. The main disadvantages of this method are the computational resources required to perform this exhaustive search and the need for a number of signal components to be known in advance; however, the frequencies found by it have a better resolution than those found by a FFT, which would have a resolution depending on the coarse-graining of its frequency bins \cite{Kootsookos1999}. The values $\omega_k$ can then be used a features for a learning procedure.

\subsection{Independent component analysis} \label{subsection_ica}
Independent Component Analysis (ICA) is a feature extraction method that assumes that a (uni- or multivariate) signal over time $\V{x}$ of length $N$ is a linear combination of weighted, statistically independent \emph{source signals} $\V{s}$ (assuming at most one of them is Gaussian), each of length $m$, in the presence of Gaussian noise $\V{n}$, i.e. $\V{x} = \V{As} + \V{n}$ \cite{Kasprzak}.
\par Depending on the application, researchers might choose to focus on studying the basis vectors from the columns of $\V{A}$ or on analysing the original signals \cite{Kasprzak,Hsieh2009,Chien2006,Jang2001}. Note that, since $m<N$, ICA can also be seen as a dimensionality reduction method: at each time $t$, an entry $\V{x}_t$ has $N$ entries that are mapped into an $m$-dimensional vector $\V{s}_t$. 
\par Iterative methods are commonly used to find the source signals by updating an unweighting matrix $\V{W}$, where $\V{s} = \V{Wx}$ \cite{Hyvarinen2000}. This is done by checking convergence based on a cost function that measures statistical independence among the current source signals. One such algorithm is FastICA, which has been shown to accelerate convergence \cite{Hyvarinen2000, Kasprzak}. 
\par Non-Gaussianity is a key aspect in ICA. A classical result from Probability theory is the Central Limit Theorem, which states that, under certain conditions, the sum of two independent random variables is ``more Gaussian" than either component. A thorough discussion on this assumption is given in \cite{Hyvarinen2000}, in which the following key remark is made: let $\V{z} = \V{A}^T\V{w}$, and assume a mixture vector $\V{x}$ behaves as desired for ICA (i.e. is actually a mixture of statistically independent components); then, $\V{y}_j = \V{w}_j\V{x} = \V{w}_j\V{As} = \V{z}^T\V{s}$. Then, $\V{y}$ is a linear combination of the original sources $\V{s}$. 
\par Since combining any two $\V{s}_i, \V{s}_j$ would lead to a ``more Gaussian" distribution than either of them, the product $\V{z}^T\V{s}$ is the least Gaussian whenever $\V{z}^T\V{s} = \V{s}_i$. Therefore, $\V{w}$ can be seen as a vector that maximises the non-Gaussianity of $\V{w}^T\V{x}$. The reader is referred to \cite{Hyvarinen2000} for a broader discussion on this matter.
\par ICA has been successfully used for feature extraction in the speech recognition domain, still lacking presence in the birdsong domain. Recent works have used ICA in different approaches. In \cite{Kasprzak}, MFCC (refer to \ref{subsection_mfcc}) feature vectors are first are calculated from windowed speech signals, and then used themselves as a time signal as ICA input; the source signals are then used as features. A similar approach is presented in \cite{Hsieh2009}, but using MFCC and delta and double delta features as input to ICA. A third approach is shown in \cite{Lee2000}, speech segments are used directly as input for ICA, and then the source signals are used as features as well.
\par One more approach, presented in \cite{Jang2001}, includes using the basis columns of the matrix $\V{A}$ as features for a signal $\V{x}$ and then measure their similarity with respect to those from another signal $\V{y}$. Finally, other works focus on deriving adaptations of the ICA algorithm that better suit differently conditioned problems. For example, in some scenarios, assuming that sources are mixed linearly cannot be justified or accepted at all. Works such as \cite{Almeida2003, Omar} have proposed alternatives for these cases.

\subsection{Linear Predictive Coding and Formant Frequencies} \label{subsection_formants}
Sufficiently small portions of speech utterances $\V{x}$ can be seen as a Linear Time-invariant (LTI) system in which a flat spectrum is produced by the glottis and then reshaped by a linear filter, the vocal tract, as explained by the source-filter model of human speech \cite{Bello}. Let us recall that any LTI system is uniquely characterised by its impulse response function, or equivalently in the frequency domain, by its frequency response \cite{markel1976}. Thus, characterising the frequency response of the system also characterises the system itself. Formant extraction consists in finding the resonances of the vocal tract of short (20-40 ms) speech utterances - a longer speech utterance can be segmented into smaller signals whose formants form a trajectory.
\par Formants are also the peaks of the spectral envelope of the analysed signal and the resonances of the vocal tract's characteristic function. Hence, by building a formant trajectory, we are describing how the vocal tract and the spectral envelope change through time.
\par Now, assume that the signal can be approximated using a Linear Predictive Coding approach of $p$-th order, i.e. $\hat{\V{x}} = \sum_{k=1}^pa_k\V{x}_{t-k}$. Then, the error at each time $\V{e}_t$ between the original signal and a $p$-th order approximation is given by: 
\begin{align*}
\V{e} = \V{x} - \hat{\V{x}} = \V{x} - \sum_{k=1}^p{a_k \V{x}_{t-k}}
\end{align*} 
\par By taking the z-transform\footnote{The z-transform is a method to compute the frequency domain representation of a discrete signal. The reader is referred to subsection \ref{subsection_lti} for a formal definition.} of this expression, we obtain:
\begin{align*}
\mathcal{Z}\{\V{e}\} &= \mathcal{Z}\left\{\V{x} - \sum_{k=1}^p{a_k \V{x}_{t-k}}\right\}\\
E(z) &= X(z)\left(1-\sum_{k=1}^p{a_k z^{-k}}\right)\\
E(z) &= X(z)A(z)\\
\implies X(z) &= \frac{E(z)}{A(z)}
\end{align*}
\par Remark that $A(z)$ is still specified by the LPC coefficients, and thus $X(z)$, a frequency domain representation of the time signal $\V{x}$, now depends directly on the LPC coefficients \cite{Hosom2006}. The poles of the above function come in complex pairs, and are given by the roots $z_i = r_i\exp{\{\pm\theta_i\}}$ of $A(z)$. The formants in Hertz and their corresponding 3-dB bandwidth are given by \cite{Snell1993,Mathworks2015}:
\begin{align*}
F_i &= \frac{f_s}{2\pi}\theta_i \text{Hz}\\
B_i &= -\frac{f_s}{\pi}\log{(r_i)} \text{Hz}
\end{align*}
\par The bandwidth is a metric to choose how distinguishing each formant is, and represents the width in Hertz of each formant peak 3-dB below the top. Narrower bandwidths represent more distinguishing formants. 
\par Other approaches to estimate formants focus on finding different ways of estimating the LPC coefficients. For example, \cite{Darch,Araujo1998} propose calculating the LPC coefficients from a spectral envelope sampled exclusively at critical frequencies in the Mel-scale.
\par Two strong arguments in favour of formant trajectories are the well-studied similarity between human vowels and birdsong and the broad literature covering applications of formant trajectory extraction for human speech processing. For example, regarding the latter, \cite{Ferragne2010} used formant trajectories to show the phonetic differences of 13 different British accents. In \cite{Holmes1895}, the authors used formants to classify utterances of digits in English and compare the results against other feature extraction techniques (such as cepstral features). Finally, formants were also used for vowel sounds classification in \cite{markel1976}.
\par Recall from section \ref{birdsong_review} that birdsong is very similar in anatomy to human vowels. Formants of human vowels have been widely studied, which suggests that similar results could be achieved for birdsong. Moreover, there is a broad literature on choosing the number of formants and the order of the underlying autoregressive model, which implies that parametrising it takes less time than doing it for wavelets. Finally, being based on a theoretical model of sound production, formants possess strong biological and computational arguments to be used.

\subsection{Information geometry} \label{subsection_infgeom}
So far, we have described several techniques that map audio recordings to their frequency domain representations and then characterise it. It often happened that signals were to be segmented into sufficiently small windows, which were then analysed via any of these techniques. Therefore, the output for a whole recording would be a vector of observations. One disadvantage of all of these methods is that recordings are almost always of different length, and thus the resulting vectors of observations do not have the same dimensionality. Most of them would also reflect the differences in the vocal tracts of the bird individuals. This justifies the search for a more abstract method that compares patterns disregarding small numerical variations between them. 
\par The framework of Information Geometry aims at overcoming these issues by applying tools from Differential Geometry to Probability Theory \cite{Wang2009}. It assumes that each object of study (e.g. a recording) belongs to a point in a Riemannian manifold, and then proceeds to define similarity metrics between these points. In the domain of acoustic signals, this means to be able to build statistical models from a signal and then define a similarity metric between these.
\par The authors of \cite{Cont2011} define a framework for the analysis of acoustic signals using Information Geometry. They discuss, for example, how sequences of features (such as MFCCs) can be used to build an exponential probability distribution and how clustering (in particular, centroid computation) and model comparison can be performed over this distribution, rather than the raw features. They also apply this framework to automatically segment audio files by detecting pattern changes in the input signal.
\par Even though Information Geometry has been studied mostly for parametric families of probability distributions \cite{Amari2001}, the key concept of having a structure that summarises the information of our objects of study and then define a similarity metric between them can be considered for other classes of models. Learning non-parametric probability distributions can be done by Kernel Density Estimation \cite{Goodall2008}, which takes a weighted combination of kernel functions as a probability distribution; other structures, such as Hidden Markov Models \cite{Chou2008,Muda2010,Hsieh2009,Wielgat2012}, have been broadly used in the Machine Learning community to model the distribution and shape of audio sequences over time. Building these statistical models as an intermediate step encourages learning feature vectors as symbols, rather than sequences of varying dimensionality.

\subsection{Conclusion}
In this section, we introduced several feature extraction techniques. Advantages and disadvantages of each method were presented, but we can conclude that they mostly rely on mapping a time-domain signal into its frequency-domain representation, and then use another signal processing technique to characterise it. However, one method that goes further and gives biological arguments in its favour is formant trajectory extraction. Although they have not been used extensively in birdsong characterisation, the fact that birdsong anatomy resembles human vowels production suggests that formant trajectory extraction can lead to similar results.
\par For these reasons, we will use formant trajectory extraction as our feature extraction method, and hence will offer a broader discussion in chapter \ref{chapter_formants}. However, we are interested in characterising these trajectories symbolically, i.e. small numerical variations should be disregarded in favour of characterising the shape of the trajectory, as discussed in section \ref{subsection_infgeom}. Methods to build this kind of abstract structures will be discussed in chapter \ref{chapter_hmms}.


\section{Building relational structures}\label{algorithms_review}
Once bird species have been characterised, a pairwise distance matrix can be built by using an appropriate metric. Appendix \ref{metrics_review} introduces metrics between pairs of vectors and pairs of probability distributions. A broader discussion is also offered in section \ref{section_similarity}.
\par This distance matrix can be used to build a relational structure of bird species. There exist several algorithms to create relational structures, and the differences between them are usually related to the purposes each of them addresses, e.g. are we building a tree or a general graph? Can a bird species belong to several clusters at the same time? Do clusters change through time? Algorithms addressing these issues are reviewed in this section.

\subsection{K-means algorithm} \label{subsection_kmeans}
The first algorithm that we cover is K-means. This is an iterative descent algorithm that partitions a purely quantitative set $X$ of $M$ feature vectors into $K$ clusters \cite{hastie2008}. The procedure is divided in two stages: we first choose centroids $\{C_i\}$ (at random for initialisation, and then by finding the mean vector of every cluster) and then create new clusters by assigning each data point to the cluster with the closest centroid $C_i$ (in terms of the Euclidean distance). This is repeated until the clusters stop changing. Pseudocode for this procedure is given in algorithm \ref{alg_kmeans}.
\begin{algorithm}
\begin{algorithmic}[1]
\Function{kmeans}{$X, K$}
\State choose $K$ points at random from $X$ as centroids
\Repeat
\ForAll{$x_i$}
    \ForAll{$c_j$}
        \State $d(i,j) = \norm{x_i - c_j}$
    \EndFor  
    \State $j' = \argmin_j{d(i, :)}$\Comment{choose centroid with minimum distance}
    \State $C(j')$.add($x_i$)
\EndFor
\ForAll{$C(j')$}
    \State set $c_j \gets \frac{1}{\abs{C(j')}}\sum_{x_i \in C(j')}{x_i}$\Comment{recompute centroids}
\EndFor  
\Until{convergence}
\EndFunction
\caption{The K-means algorithm}\label{alg_kmeans}
\end{algorithmic}
\end{algorithm}
\par Despite its simplicity, K-means has some disadvantages, such as being sensitive to the order of presentation of data, e.g. if two centroids are equally apart from a single data point $x_i$, then it will be assigned to the cluster that is presented last. Moreover, K-means only gives a snapshot of the data at a specific point in time - it doesn't describe how the links are formed over time. 

\subsection{Agglomerative hierarchical clustering} \label{subsection_hier}
This is one of the traditional methods for detecting community structure \cite{Girvan2002}, whose output is an arborescent structure created by progressively linking clusters. We begin assuming that each point in the dataset is a singleton cluster, and in each iteration we create a new cluster by merging the two closest clusters \cite{hastie2008}. We remark that the input to this algorithm is a similarity matrix (rather than feature vectors, as in K-means) and that, every time we merge a cluster, we only have to update the distances between the newly created cluster and the rest. 
\par The simplest linking technique is \emph{single linkage}, which takes the inter-cluster distance to be the one between the most similar pair, i.e. for clusters $C_1, C_2$, their single linkage distance is given by:
\begin{align*}
d_{SL} = \min_{x_i\in C_1, x_i'\in C_2}{d_{i,i'}}
\end{align*}
\par The procedure stops when there is only one cluster containing all data. The output of hierarchical clustering is a list of triples $(C_i, C_i', d_{i,i'})$ that can be seen as records of clusters that were merged together, and how far apart they were from each other. This list of tuples can then be visualised as a dendrogram \cite{HAGMathworks}, which is the tree-like structure depicted in figure \ref{fig_hag}. It is important to notice that agglomerative approaches possess a \emph{monotonicity} property: the dissimilarity between merged clusters is monotone increasing with the level of the merger. If this were not the case, a dendrogram representation of the data would not be possible \cite{hastie2008} - as it is the case for some divisive approaches, discussed in subsection \ref{subsection_divisive}. A broader discussion of this technique and other linking strategies can be found in chapter \ref{chapter_hmms}.
\par As opposed to K-means, hierarchical clustering shows an evolution of clusters over time, so it can be used to model both, how communities cluster together as times flows, but also to take a closer look at how a specific community looked like at a specific moment in time.
\begin{figure}[t]
\centering
\includegraphics[width=80mm]{dendrogram}
\caption{A dendrogram with 11 leaves. The horizontal axis is the distance between the merged clusters; the vertical axis is the identifier of the leaves.}
\label{fig_hag}
\end{figure}

\subsection{Divisive hierarchical clustering} \label{subsection_divisive}
This approach can be seen as the counterpart of agglomerative clustering: rather than starting with $\abs{X}$ clusters, the algorithm considers the whole set $X$ as a single cluster and then splits it iteratively. Some results have shown that this paradigm can produce more accurate results in a shorter amount of time than agglomerative clustering, although there has not been as much research about it \cite{Manning2009}.  
\par In the simplest case, K-means is applied iteratively to the dataset, i.e. we partition the input cluster into 2 subclusters using $K=2$, and then repeat this procedure for each cluster. However, this approach lacks the monotonicity property described in subsection \ref{subsection_hier}, and hence a consistent dendrogram representation is not guaranteed \cite{hastie2008}.
\par Further approaches to divisive clustering include the one proposed by Macnaughton Smith in 1965 \cite{hastie2008}, which proceeds as follows: at each iteration, choose the element from input cluster $G$ whose average distance from the rest $G' = G - \{x_i\}$ is largest. The average dissimilarity of $x_i$ with respect to $G'$ is given by:
\begin{align*}
\bar{d}_{G'}(x_i) = \frac{1}{\abs{G'}}\sum_{i'\in G'} d_{i,i'}
\end{align*}
\par This is the seed of a new cluster, $H$. For each successive step, add to $H$ the element $x_i$ such that:
\begin{align*}
\argmax_{x_i \in X} \bar{d}_H(x_i) - \bar{d}_{G-\{x_i\}}(x_i)
\end{align*}
\par This continues until the expression above becomes negative, in which case the algorithm is now applied recursively to $G$ and $H$. This means that there is no other element in $G$ that is closer to $H$ in average \cite{hastie2008}. A broader discussion of divisive hierarchical clustering splitting techniques can be found in \cite{.2006}.

\subsection{Girvan-Newman algorithm} \label{subsection_gn}
This algorithm was first proposed in \cite{Girvan2002}, and focuses on analysing the edges of the network (rather than the vertices). In particular the algorithm chooses edges that lie \emph{between} clusters-to-be, and removes them. This metric is called \emph{edge betweenness}, and is defined as the number of shortest paths between pairs of vertices that run along it \cite{Girvan2002}. Edges $e\in E$ with greater betweenness are likely to connect communities, and thus removing them encourages the partitioning of vertices \cite{Lu2012}.
\begin{algorithm}
\begin{algorithmic}[1]
\Function{gncommunity}{$X, K$}
\Repeat
\ForAll{edges $e \in E$}
    \State Calculate betweenness $b(e)$
\EndFor
\State remove $\argmax_e{b(e)}$
\Until{no edges have been affected by the last removal}
\EndFunction
\caption{The GN algorithm for community detection.}\label{alg_gn}
\end{algorithmic}
\end{algorithm}
\par The main disadvantage of this approach is that it does not handle weighted graphs, thus distance matrices cannot be used to find inner communities.

\subsection{Soft-partitioning} \label{subsection_softpart}
All the approaches presented until now assume that a single data point can only belong to one cluster or community. However, it is often the case that communities overlap, i.e. that there are at least two clusters $C_i, C_j$ such that $C_i \cap C_j \neq \emptyset$. An approach to solve this problem is proposed in \cite{Psorakis2010}, and relies on matrix factorisation techniques.
\par Let $\V{V}$ be a matrix of interactions where $v_{i,j}$ denotes the number of interactions between objects $x_i, x_j$. Then, we can model $\hat{\V{V}}$ as the product of two rank $K$ matrices $\V{W} \in \mathbb{R}_+^{N \times K}, \V{H}\in \mathbb{R}_+^{K \times N}$, i.e. $\hat{\V{V}} = \V{WH}$, where $K$ is the number of hidden communities in the network. The product $w_i^Th_j$ can be seen as the \emph{expected number of interactions} between two individuals $x_i, x_j$ - their mutual participation \cite{Psorakis2010}. However, the number of communities $K$ is usually unknown. The authors address this issue by placing automatic relevance determination priors $\V{B} = \text{diag}(\beta_k)$ on the latent variables $w_{ik}, h_{k, j}$ using a Gamma distribution with parameters $a, b$ \cite{Psorakis2010a}.
\par The parameters of this model can be found iteratively via algorithm \ref{alg_nmf} from \cite{Psorakis2010a}:
\begin{algorithm}
\begin{algorithmic}[1]
\Function{nmf}{$V, K_0, a, b$}
\Repeat
    \State $\V{H} \gets \frac{\V{H}}{\V{W}^T\V{1}+\V{BH}}\V{W}^T\V{\frac{\V{V}}{\V{WH}}}$
    \State $\V{W} \gets \frac{\V{W}}{\V{1}\V{H}^T+\V{WB}}\V{\frac{\V{V}}{\V{WH}}}\V{H}^T$
    \State $\beta_k \gets \frac{N-a-1}{1/2(\sum_i{w_{ik}^2} + \sum_j{h_{kj}^2})} + b$
\Until{$n_iter$}
$K_f \gets $ number of non-zero columns of $\V{W}$ or rows of $\V{H}$
\State \textbf{return} $\V{W}_f \in \mathbb{R}_+^{N\times K_f}, \V{H}_f\in\mathbb{R}_+^{K_f\times N}$
\EndFunction
\caption{The GN algorithm for community detection.}\label{alg_nmf}
\end{algorithmic}
\end{algorithm}
\par Details on the derivation of these update rules can be found in \cite{Psorakis2010,Psorakis2010a}. 
\par The main advantage of this method is that it accounts for overlapping communities and finds an approach that works well over weighted graphs (an aspect lacking from the approach in subsection \ref{subsection_gn}. Work to incorporate other factors in community detection, such as time and geographical dynamics is under way \cite{Psorakis2010}.
\par However, similar to K-means, this method only allows us to see communities at a single instant in time, i.e. we do not see how communities (be them hard- or soft-partitioned) form as time flows. Therefore, this method is appropriate to use when we want to analyse communities, rather than the dynamics of the community.

\subsection{Conclusion}
In this section, we reviewed several relational structure building algorithms. Each of them models a different kind of structure, and hence the choice of what algorithm to use depends heavily on what we are aiming to model. In this project, we aim to provide evidence of how bird species acoustics could have changed through time, i.e. we aim to progressively cluster bird species to simulate how birdsong could have diversified. This task is better suited for the hierarchical clustering algorithms described in subsections \ref{subsection_hier} and \ref{subsection_divisive}. 
\par In particular, the agglomerative approach guarantees the existence of a visual representation of the relational structure, hence providing a straightforward way to make a qualitative assessment of the results. For these reasons, we give a deeper presentation of this algorithm in section \ref{section_hierarchical} and use it to generate our phylo-acoustic tree.

\section{Conclusion}
In this chapter we introduced a general pipeline to build relational structures, with three main processes: characterisation, distance matrix computation, and structure generation. Then, we described general aspects of birdsong that provide insight to choose appropriate techniques for later stages of the process. For each of the subsequent sections, we presented several techniques to fill in the gaps of the general pipeline described in section \ref{general_pipeline}.
\par From section \ref{features_review}, we learnt that one technique that is capable of modelling the dynamics of vocal tracts are formants. Although not much research of formants applied to birdsong has been done, this project could set some preliminary results to test whether they would be a faithful representation of birdsong data. Hence, formants and a signal analysis review to expand on them are covered in chapter \ref{chapter_formants}.
\par Additionally, in section \ref{features_review}, we also learnt that using an intermediate statistical model can serve to summarise the content and structure of data and encourage the learning of feature vectors as symbols, rather than as sequences of different lengths. In chapter \ref{chapter_hmms}, we further discuss the details on how to build this kind of models.
\par We also introduced several relational structure learning algorithms, and highlighted agglomerative hierarchical clustering as one that can output arborescent structures that describe community dynamics over time. A more profound treatment of this algorithm is offered in chapter \ref{chapter_hmms} as well. 
\par To conclude, we have seen that although there is a limited amount of research on birdsong community detection, solutions can be approached by filling the gaps of a generic pipeline - this means that this literature review can serve as a basis for work to be done beyond this dissertation.
\end{document}